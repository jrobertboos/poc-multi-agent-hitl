version: '2'
image_name: llamastack-poc-multi-agent-hitl
container_image: null

external_providers_dir: ../providers

distribution_spec:
  local:
    services:
      - agents

apis:
  - inference
  - telemetry
  - agents
  - vector_io
  - safety
  - tool_runtime

providers:
  agents:
    - provider_id: hitl-agent
      provider_type: inline::hitl-agent
      config:
        persistence_store:
          type: sqlite
          db_path: ./kvstore.db
        responses_store:
          type: sqlite
          db_path: ./responses.db
        hil_endpoint: "http://localhost:8005/"
  tool_runtime:
    - provider_id: model-context-protocol
      provider_type: remote::model-context-protocol
      config: {}
  inference:
    - provider_id: openai
      provider_type: remote::openai
      config:
        api_key: ${env.OPENAI_API_KEY}
  telemetry:
    - provider_id: meta-reference
      provider_type: inline::meta-reference
      config:
        sinks: ['console']
  safety:
    - provider_id: quota-limiter
      provider_type: inline::quota-limiter
      config: {
        "db_path": "../quota.txt",
        "inital_quota": 100,
      }

tool_groups:
  - toolgroup_id: mcp::multi-agent
    provider_id: model-context-protocol
    mcp_endpoint:
      uri: "http://localhost:8000/sse"
  - toolgroup_id: mcp::quota
    provider_id: model-context-protocol
    mcp_endpoint:
      uri: "http://localhost:8001/sse"

models:
  - model_id: gpt-4-turbo
    provider_id: openai
    model_type: llm
    provider_model_id: gpt-4-turbo
